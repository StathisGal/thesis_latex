% \documentclass{report}

% \usepackage{subcaption} % package for subfigures
% \usepackage{hyperref}  % package for linking figures etc
% \usepackage{enumitem}  % package for description with bullets
% \usepackage{graphicx}  % package for importing images
% \usepackage{mathtools} % package for math equation
% \usepackage{mathrsfs}  % package for math font
% \usepackage{indentfirst} % package for getting ident after section or paragraph
% \usepackage[export]{adjustbox}
% \usepackage{multirow}  % package for tables, multir
% \usepackage{amssymb}
% % \usepackage{tabu}   % for tables 
% \setlength{\parindent}{2em} % how much indent to use when we start a paragraph

% \graphicspath{ {./theory/figures/} }       % path for images

% \begin{document}

\chapter{Tube Proposal Network}

\section{ Our implementation's architecture}
In this chapter, we get involved with Tube Proposal Network(TPN), one of the basic elements of ActionNet. Before describing it, we present
the whole structure of our model. We propose a network similar to \cite{DBLP:journals/corr/HouCS17}.
Our architecture is consisted by the following basic elements:
\begin{itemize}
\item One 3D Convolutional Network, which is used for feature extraction. In our implementation we use a 3D Resnet network whose implementation  is taken from   \cite{hara3dcnns} and it is based on ResNet CNNs for Image Classification (\cite{DBLP:journals/corr/HeZRS15}).
\item Tube Proposal Network for proposing ToIs (based on the idea presented in \cite{DBLP:journals/corr/HouCS17}).
\item A classifier for classifying proposed action video tubes.
\end{itemize}

The basic procedure ActionNet follows is:
\begin{enumerate}
\item Given a video, we separate it into video segments. These video segments in some cases overlap temporally and in some others don't.
\item For each video segment, after performing spatiotemporal resizing, we feed its frames into ResNet34 in order to perform feature
  extraction. These activation maps are, next, fed into TPN for proposing sequences of bounding boxes. We name these sequences as Tubes of Interest (ToIs), 
  like \cite{DBLP:journals/corr/HouCS17} did because they are likely to contain a person performing an action.
\item After getting proposed ToIs for each video segment, using a linking algorithm, ActionNet finds final candidate action tubes. These
  action tubes are given as input to a classifier in order to get their action class.
\end{enumerate}

A diagram of ActionNet is shown at Figure \ref{fig:whole_network_}.

\begin{figure}[h]
  % \includegraphics[scale=0.7]{convolutional_neural_network_structure} \]
  \centering
  \includegraphics[scale=0.42]{model_prenms}
  \caption{The structure of the whole network}
  \label{fig:whole_network_}
\end{figure}

\section{Introduction to TPN}
 The main purpose of Tube Proposal Network (TPN)  is to propose
\textbf{Tube of Interest}(TOIs). These tubes are likely to contain an known action and are consisted of some 2D boxes
(1 for each frame). TPN is inspired from RPN introduced by FasterRCNN (\cite{Ren:2015:FRT:2969239.2969250}), but instead of images, TPN
is used in videos as performed by \cite{DBLP:journals/corr/HouCS17}. In full correspondence with RPN, the structure
of TPN is similar to RPN. The only difference, is that TPN uses 3D Convolutional Layers and 3D anchors instead of 2D. \par
We designed 2 main structures for TPN. Each approach has a different definition of the used 3D anchors.
The rest structure of the TPN is mainly the same with some little differences in the regression layer. \par

\section{Preparation before TPN}

\subsection{Preparing data}
Before getting a video as input to extract its features and ToIs, this video has to be preprocessed.
Preprocess procedure  is the same for both approaches of TPN.
Our architecture gets as input a sequence of frames which has a fixed  width, height and duration. However, each video has a different resolution. That's creates the
need to resize each frame before feeding it to the architecture.
As mentioned in the previous chapter, the first element of our network is a 3D ResNet taken from \cite{hara3dcnns}. This network is designed to
get images with dimensions (112,112). As a result, we resize each frame from datasets' videos into (112,112) frames. In order to keep aspect ratio, we pad each frame either
left and right, either above and bellow depending which dimension is bigger. In figure  \ref{fig:Preprocess_example} we can see the original frame and the resize and padded one.
In full correspondence, we resize the groundtruth bounding boxes for each frame (figure \ref{fig:original_image_rois} and \ref{fig:trans_image_rois} show that).

\begin{figure}[h]
  \centering
  \begin{subfigure}{0.35\textwidth}
    \includegraphics[width=\textwidth]{./figures/original_image.jpg}
    \caption{}
    \label{fig:original_image}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.35\textwidth}
    \includegraphics[width=\textwidth]{./figures/original_image_rois.jpg}
    \caption{}
    \label{fig:original_image_rois}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.35\textwidth}
    \includegraphics[width=\textwidth]{./figures/transformed_image.jpg}
    \caption{}
    \label{fig:trans_image}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.35\textwidth}
    \includegraphics[width=\textwidth]{./figures/transformed_image_rois.jpg}
    \caption{}
    \label{fig:trans_image_rois}
  \end{subfigure}

  \caption{ At (a), (b) frame is its original size and at (c), (d) same frame after preprocessing part}
  \label{fig:Preprocess_example}
\end{figure}


\subsection{3D ResNet}
Before using the Tube Proposal Network, we extract spatiotemporal features of the video. In order to do so, we extract the 3 first Layers of a
pretrained 3D ResNet34. This Network is pretrained in Kinetics dataset \cite{DBLP:journals/corr/KayCSZHVVGBNSZ17} for sample duration
equal to 16  frames and sample size equal to (112, 122). \par
This network normally is used for classifying the whole video, so some of its layers use temporal stride equal to 2.
We set their temporal stride equal to 1 because we don't want to miss any temporal information during the process.
So, the output of the third layer is a feature maps with dimensions (256,16,7,7). We feed this feature map to TPN, which is described
in the following sections.

\section{ 3D anchors as 6-dim vector}
\subsection{First Description}
We started designing our TPN inspired by \cite{DBLP:journals/corr/HouCS17}. We consider each anchor as a 3D bounding box written as
$(x_1, y_1, t_1, x_2, y_2, t_2)$ where $x_1, y_1, t_1$
are the upper front left coordinates of the cuboid and $x_2, y_2, t_2$ are the lower back left as shown in figure \ref{fig:anchor_6d}.
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{anchor_6d}
  \caption{An example of the anchor $(x_1,y_1,t_1,x_2,y_2,t_2)$}
  \label{fig:anchor_6d}
\end{figure}

The main advantage of this approach is that, except from x-y dims, the dimension of time is mutable. As a result, the proposed TOIs have
no fixed time duration. This will help us deal with untrimmed videos, because proposed TOIs would exclude background frames.
For this approach, we use \textbf{n = 4k = 60} anchors for each pixel in the feature map of TPN. We have k anchors for each anchor 
duration( 5 scales of 1, 2, 4, 8, 16, 3 aspect ratios of 1:1, 1:2, 2:1 and 4 durations of 16,12,8,4 frames).
In \cite{DBLP:journals/corr/HouCS17},  network's anchors are defined according to the dataset most common anchors. This, however,
creates the need to redesign the network for each dataset. In our approach, we use the same anchors for both datasets, because we want our network not
to be dataset-specific but to be able to generalize for several datasets. As sample duration, we chose 16 frames per video segment because
our pre-trained ResNet is trained for video clips with that duration.
So the structure of TPN is:
\begin{itemize}
\item 1 3D Convolutional Layer with kernel size = 3, stride = 3 and padding = 1
\item 1 classification layer outputs \textit{2n scores,} whether there is an action or not for \textit{n tubes}.
\item 1 regression layer outputs \textit{6n coordinates} ($x_1,y_1,t_1,x_2,y_2,t_2$) for \textit{n tubes}.
\end{itemize}

The structure of TPN is shown in figure \ref{fig:tpn_1_1}. The output of TPN is the k-best scoring cuboid,which are most likely to contain an action.
\begin{figure}[h]

  \includegraphics[width=1.\textwidth]{tpn_1_1}
  \caption{Structure of TPN}
  \label{fig:tpn_1_1}
\end{figure}

\subsection{Training}
As mentioned before, TPN extracts TOIs as 6-dim vectors. For that reason, we modify out groundtruth ROIs to groundtruth Tubes.
We take for granted that the actor cannot move a lot during 16 frames, so that's why we use this kind of tubes. As shown 
in figure \ref{fig:gt_tubes_and_rois}, these tubes are 3D boxes which include all the groundtruth rois, which are different
for each frame.

\begin{figure}[h]
  \centering
  \begin{subfigure}{0.15\textwidth}
    \includegraphics[width=\textwidth]{output/img_0.jpg}
  \end{subfigure}
  \begin{subfigure}{0.15\textwidth}
    \includegraphics[width=\textwidth]{output/img_3.jpg}
  \end{subfigure}
  \begin{subfigure}{0.15\textwidth}
    \includegraphics[width=\textwidth]{output/img_5.jpg}
  \end{subfigure}
  \begin{subfigure}{0.15\textwidth}
    \includegraphics[width=\textwidth]{output/img_7.jpg}
  \end{subfigure}
  \begin{subfigure}{0.15\textwidth}
    \includegraphics[width=\textwidth]{output/img_11.jpg}
  \end{subfigure}
  \begin{subfigure}{0.15\textwidth}
    \includegraphics[width=\textwidth]{output/img_15.jpg}
  \end{subfigure}
  \caption{Groundtruth tube is colored with blue and groundtruth rois with color green}
  \label{fig:gt_tubes_and_rois}
\end{figure}

For training procedure, for each video, we randomly select a part of it which has duration 16 frames. We consider an anchor as foreground if its overlap score with a groundtruth
action tube is bigger than 0.5. Otherwise, it is considered as background anchor. We use scoring layer in order to correctly classify those anchors and we use
Cross Entropy Loss as loss function. We have a lot of anchors for proposing an action, but a small number of per-video actions, so we choose 256 anchors in total for each batch. We set the maximum
number of foreground anchors to be  25\% of the 256 anchors and the rest are the background.\par
Classifying correctly an anchor isn't enough for proposing an action tube. It is,also necessary, the anchors  overlap as much as possible with the groundtruth action tubes. That's the reason we use a
regression layer. This layer ``moves'' the cuboid closer to the area that it is believed that is closer to the action.
For regression loss we use smooth-L1 loss as proposed from \cite{DBLP:journals/corr/GirshickDDM13}. In order to calculate
the regression targets, we use pytorch FasterRCNN implementation (\cite{jjfaster2rcnn}) for bounding box regression and 
we modified the code in order to extend it for 3 dimensions. % \textbf{TODO more details}
So we have:
\[ \begin{matrix}
    t_x = (x-x_a)/w_a, & t_y = (y-y_a)/h_a, & t_z= (z-z_a)/d_a, \\
    t_w= log(w/w_a), & t_h= log(h/h_a), & t_d = log(d/d_a), \\
    t^*_x = (x^* - x_a)/w_a, & t^*_y = (y^* - y_a)/h_a, & t^*_z = (z^* - z_a)/d_a, \\
    t^*_w = log(w^* /w_a), & t^*_h = log(h^*/h_a), & t^*_d = log(d^*/d_a),
    % t∗x= (x∗−xa)/wa,  t∗y= (y∗−ya)/ha,t∗w= log(w∗/wa),  t∗h= log(h∗/ha)
  \end{matrix}
\]
where \textit{x, y, z, w, h, d} denote the 3D box's center coordinates and its width, height and duration. Variables $x, x_a, \text{ and } x^*$
are for the predicted box, anchor box, and groundthruth box respectively (likewise for \textit{y, z, w, h, d}). Of course, we calculate the
regression loss only for the foreground anchors and not for the background, so at the most we will calculate 64 targets
for each batch. \par

To sum up training procedure, we train 2 layers for our TPN, scoring and regression layers. The training loss includes the training losses
obtained by these layers and its formula is:
% \[ L  = \frac{1}{N_{cls}} \sum_iL_{cls}(p_i, p^*) + \frac{1}{N_{reg}}\sum_ip_i^*L_{reg}(t_i,t_i^*) \]
\[ L  =  \sum_iL_{cls}(p_i, p_i^*) + \sum_ip_i^*L_{reg}(t_i,t_i^*) \]
where:
\begin{itemize}
\item $L_{cls} $ is the Cross Entropy loss we use for classifying the anchors, with $p_i$ is the predicted label, $p_i^*$ is the groundtruth class and
  $p_i, p_i^* \in \{0,1\}$
\item $L_{reg} $ is the smooth-L1 loss function, which is multiplied  with $p_i^*$ in order to be set active only when there is a positive anchor $(p_i^* = 1)$
  and to be deactivated for background anchors $(p_i^* = 0)$.
\end{itemize}

\subsection{Validation}

Validation procedure is a bit similar to training procedure.
We randomly select 16 frames from a validation video and we examine if there is at least 1 proposed TOI
which overlaps $\ge$ 0.5 with each groundtruth action tube and we get recall score. 
In order to get good proposals, after getting classification scores and target prediction from the
corresponding layers, we use Non-Maximum Suppression (NMS) algorithm.  We set NMS threshold equal to 0.7,
and we keep the first 150 cuboids with the biggest score.

\subsection{Modified Intersection over Union(mIoU)} 
During training, we get numerous anchors. We have to classify them as foreground anchors or
background anchors. Foreground anchors are those which contain some action, and, respectively, background
don't. As presented before, IoU for cuboids calculates the ratio between the volume of overlap and volume of
union.
Intuitively, this criterion is good for evaluating 2 tubes if they overlap, but it has one big drawback:
it considers x-y dimensions to have the same importance with time dimension, which we do not desire. That's because
firstly we care to be accurate in time dimension, and then we can fix x-y domain.
As a result, we change the way we calculate the Intersection Over Union. We calculate separately
the IoU in x-y domain (IoU-xy) and in t-domain (IoU-t). Finally, we multiply them in order to get the final IoU.
So the formula for 2 tubes ($x_1, y_1, t_1, x_2, y_2, t_2$) and ($x'_1, y'_1, t'_1, x'_2, y'_2, t'_2$) is:
\[ IoU_{xy} = \frac{ \text{Area of Overlap in x-y}} { \text{Area of Union in x-y}}  \]
\[ IoU_t = \frac { max(t_1, t'_1) - min(t_2, t'_2)} {min(t_1,t'_1) - max(t_2,t'_2)} \]
\[ IoU = IoU_{xy} \cdot  IoU_t \]
The above criterion help us balance the impact of time domain in IoU. For example, let us consider 2 anchors:
a = (22, 41, 1, 34, 70, 5) and b = (20, 45, 2, 32, 72, 5). These 2 anchors in x-y domain have IoU score equal to 0.61.
But they are not exactly overlapped in time dimension. Using the first approach we get 0.5057 IoU score and using the
second approach we get 0.4889. So, the second criterion would reject this anchor, because there is a difference in time
duration.  \par

In order to verify our idea, we train TPN using both IoU and mIoU criterion for tube-overlapping. At Table \ref{table:iou_miou}
we can see the performance in each case for both datasets, JHMDB and UCF. The recall threshold for this case is 0.5 and during validation,
we use regular IoU for defining if 2 tubes overlap.
\begin{table}[h]
\centering
  \begin{tabular}{|| c | c || c ||}
    \hline
    \textbf{Dataset} & \textbf{Criterion} & \textbf{Recall(0.5)} \\
    \hline  \hline
    \multirow{2}{4em}{JHMDB} & IoU & 0.70525 \\
    \cline{2-3}
    {} & mIoU & 0.7052 \\
    \hline
    \multirow{2}{4em}{UCF} & IoU & 0.4665 \\
    \cline{2-3}
    {} & mIoU & 0.4829 \\
    \hline      
  \end{tabular}
  \caption{Recall results for both datasets using IoU and mIoU metrics}
  \label{table:iou_miou}
\end{table}

Table \ref{table:iou_miou} shows that modified-IoU give us slightly better recall performance only in UCF dataset. That's reasonable, because JHMDB dataset
uses trimmed videos so time duration doesn't affect a lot. So, from now own, during training we use mIoU as overlapping scoring policy.

\subsection{Improving TPN score}
After first tests, we came with the idea that in a video lasting 16 frames, in time domain, all kinds of actions can be separated into the following categories:
\begin{enumerate}
\item The action starts in the n-th  frame and finishes after the 16th frame of the sampled video.
\item The action has already begun before the 1st frame of the video and ends in the n-th frame.
\item The action has already begun before the 1st frame of the video and finishes after the 16th video frame.
\item The action starts and ends in that 16 frames of the video.
\end{enumerate}

On top of that, we noticed that most of actions, in our datasets, last more that 16 frames. So, we came with the idea to add  1 scoring layer and 1 regression layer
which will propose ToIs with fixed duration equal to the sample duration (16 frames) and they will take into account the spatial information produced by activation maps.
The new structure of TPN is shown in figure \ref{fig:tpn_1_2}. After getting proposals from both scores, we concat them with ratio 1:1 between ToI extracted
from those 2 subnetworks.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{tpn_1_2}
  \caption{TPN structure after adding 2 new layers, where k = 5n.}
  \label{fig:tpn_1_2}
\end{figure}
Our goal is to ``compress'' feature maps in the temporal dimension in order to propose ToIs based only on the spatial information.
So, we came with 2 techniques for doing such thing:
\begin{enumerate}
\item Use 3D Convolutional Layers with kernel size = (sample duration, 1,1), stride =1 and no padding for scoring and regression.
  This kernel ``looks'' only in the temporal dimension of the activation maps and doesn't consider any spatial dependencies.
\item Get the average values from temporal dimension and then use a 2D Convolutional Layer for scoring and regression.
\end{enumerate}

% \textbf{TODO na perigrapsw oti thelw na exw ola ta xronika features}
\par
Training and Validation procedures remain the same. The only big difference is that now we have losses obtained from 2 different systems which propose TOIs. On top of that,
during validation, we,at first, concate proposed ToIs and, then, we follow the same procedure, which is calculating recall performance. For training loss, we have 2 different cross-entropy losses and 2 different smooth-L1 losses, each for every
layer correspondingly. So training loss is, now, defined as :

  % \begin{aligned}
 % L  =  \sum_iL_{cls}(p_i, p_i^*) + \sum_iL_{cls}(p_{fixed,i}, p_{fixed,i}^*) +  
 %  \sum_ip_i^*L_{reg}(t_i,t_i^*) + \sum_ip_{fixed,i}^*L_{reg}(t_{fixed,i},t_{fixed,i}^*) 
% L  =2


% \[ L  =  \sum_iL_{cls}(p_i, p_i^*) + \sum_iL_{cls}(p_{fixed,i}, p_{fixed,i}^*) +  \newline
%   \sum_ip_i^*L_{reg}(t_i,t_i^*) + \sum_ip_{fixed,i}^*L_{reg}(t_{fixed,i},t_{fixed,i}^*) \]
% \end{aligned}


\begin{equation} 
\begin{split}
 L  =  \sum_iL_{cls}(p_i, p_i^*) + \sum_iL_{cls}(p_{fixed,i}, p_{fixed,i}^*) + \\
   \sum_ip_i^*L_{reg}(t_i,t_i^*) + \sum_ip_{fixed,i}^*L_{reg}(t_{fixed,i},t_{fixed,i}^*) 
  % \sum_iL_{cls}(p_i, p_i^*) + \sum_iL_{cls}(p_{fixed,i}, p_{fixed,i}^*) 
\end{split}
\end{equation}

where:
\begin{itemize}
  \item $L_{cls} $ is the Cross Entropy loss we use for classifying the anchors, with $p_i$ is the predicted label, $p_i^*$ is the groundtruth class and
  $p_i, p_i^* \in \{0,1\}$
\item $L_{reg} $ is the smooth-L1 loss function, which multiply it with $p_i^*$ in order to set active only when we have a positive anchor $(p_i^* = 1)$
  and to be deactivated for background anchors $(p_i^* = 0)$.
\item $p_i $ are the anchors from scoring and regression layers with mutable time duration and $p_i^*$ are their corresponding groundtruth label.
\item $p_{fixed,i} $ are the anchors from scoring and regression layers with fixed time duration = 16 and $p_{fixed,i}^*$ are their corresponding groundtruth label.
\end{itemize}

We train our TPN Network using both techniques and their recall performance is shown in Table \ref{table:add_16}.

\begin{table}[h]
  \centering
  \begin{tabular}{||c | c | c || c ||}
    \hline
    \textbf{Dataset} & \textbf{Fix-time anchors} & \textbf{Type} & \textbf{Recall(0.5)} \\
    \hline  \hline
    \multirow{3}{4em}{JHMDB} & No &  - & 0.7052 \\
    \cline{2-4}
    {} & \multirow{2}{*}{Yes} & Kernel & 0.6978 \\
    \cline{3-4}
    {} & {} & Mean & 0.7463 \\
    \hline
    \multirow{3}{4em}{UCF} & No & - & 0.4829 \\
    \cline{2-4}
    {} & \multirow{2}{*}{Yes} & Kernel & 0.4716 \\
    \cline{3-4}
    {} & {} & Mean & 0.4885 \\
    \hline      
  \end{tabular}
  \caption{Recall results after adding fixed time duration anchors}
  \label{table:add_16}
\end{table}

As we can see from the previous results, the new layers increased recall performance significantly. On top of that, Table \ref{table:add_16} shows that
getting the average values from the time dimension gives us the best results.


\subsection{Adding regressor}
The output of TPN is the $\alpha$-highest scoring anchors moved according to their regression prediction. After that, we have to turn the proposed anchors into ToIs.
In order to do so, we add a regressor system which gets as input cuboids' feature maps and returns a sequence of 2D boxes, one per every frame.
The only problem is that the regressor needs as input feature maps with fixed size . This problem is already solved by R-CNNs which use roi pooling and roi align
in order to get fixed size feature maps from ROIs with changing sizes. In our situation, we extend roi align operation, presented by Mask R-CNN(\cite{DBLP:journals/corr/HeGDG17}),
and we
call it \textbf{3D Roi Align}.

\paragraph{3D Roi Align}
3D Roi align is a modification of roi align presented by Mask R-CNN . The main difference between those two is that Mask R-CNN's roi align uses
bi-linear interpolation for extracting ROI's features and ours 3D roi align uses trilinear interpolation for the same reason. Again, the 3rd dimension is
time.
So, we have as input a feature map extracted from ResNet34 with dimensions (64,16,28,28) and a tensor containing the proposed TOIs.
For each TOI whose activation map has size equal to (64,16,7,7), we get as output a feature map with size (64, 16, 7, 7). \par

\subsubsection{Regression procedure}
At first, for each proposed ToI, we get its corresponding activation maps using 3D Roi Align. These features are given as input to a regressor. This regressor returns 16 $\cdot$ 4 predicted
transforms $(\delta_x,\delta_y, \delta_w,\delta_h)$, 4 for each frame, where $ \delta_x, \delta_y$ specify the coordinates of proposal's center and $\delta_w, \delta_h$ its width and height, as specified
in \cite{DBLP:journals/corr/GirshickDDM13}.  We keep only the predicted translations, for the frames that are $\ge t_1$ and $< t_2$ and for the other frames, we set a zero-ed 2D box. 
After that, we modify each anchor from a cuboid written like $(x_1,y_1,t_1, x_2, y_2, t_2)$ to a sequence of 2D boxes, like: \\
$(0,0,0,0, ..., x_{T_1},y_{T_1},x'_{T_1},y'_{T_1}, ... ,x_{i},y_{i},x'_{i}, ..., x_{T_2},y_{T_2},x'_{T_2},y'_{T_2}, 0,0,0,0, ....)$, \\
where:
\begin{itemize}
\item $ T_1 \le i \le T_2$, for $T_1 < t_1 + 1,  T_2 < t_2 \text{ and }T_1,T_2 \in \mathbb{Z} $
\item $ x_i = x_1, y_i= y_1, x'_i = x_2, y'_i = y_2 $.
\end{itemize}

\paragraph{ Training}
In order to train our Regressor, we follow about the same steps followed previously for previous TPN's training procedure. This means that
we randomly pick 16 ToI from those proposed by TPN's scoring layer. From those 16 tubes, 4 are foreground tubes, which means 25\% of the total
number of the tubes as happened previously. We extract their corresponding features using 3D Roi Align and calculate their targets like
we did for regression layer. We feed Regressor Network with these features and compare the predicted targets with the expected.
Again, we use smooth-L1 loss for loss function, calculated only for foreground ToIs. So, we add another parameter in
training loss formula which is now defined as:
% \textbf{TODO Describe training procedure}
% \textbf{TODO Training Loss formula}
\begin{equation} 
\begin{split}
 L  =  \sum_iL_{cls}(p_i, p_i^*) + \sum_iL_{cls}(p_{fixed,i}, p_{fixed,i}^*) + \\
 \sum_ip_i^*L_{reg}(t_i,t_i^*) + \sum_ip_{fixed,i}^*L_{reg}(t_{fixed,i},t_{fixed,i}^*) + \\
  \sum_iq_i^*L_{reg}(c_{i}, c_{i}^*) + \\
  % \sum_iL_{cls}(p_i, p_i^*) + \sum_iL_{cls}(p_{fixed,i}, p_{fixed,i}^*) 
\end{split}
\end{equation}
where  except the previously defined parameters, we set  $c_{i} $ as the regression targets for picked tubes $q_i$.
These tubes are the ones randomly selected from the proposed ToIs and $q_i^*$ are their corresponding groundtruth action tubes, which are the closest to each $q_i$ tube.
Again, we use $q_i^*$ as a factor because we consider a tube as background when it doesn't overlap with any groundtruth action tube more that 0.5 .

\subsubsection{First regression Network} 

The architecture of reggression network is shown in Figure \ref{fig:regressor_3d}, and it is described below:
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.48]{regressor_1_1}
  \caption{Structure of Regressor}
  \label{fig:regressor_3d}
\end{figure}

\begin{enumerate}
\item Regressor is consisted, at first, with a 3D convolutional layer with kernel = 1, stride = 1 and no padding. This layer gets as input ToI's normalized activation map extracted from 3D Roi Align.
\item After that, we calculate the average value in time domain, so from a feature map with dimensions (64,16,7,7), we get as output a feature map (64,7,7).
\item These feature maps are given as input to a Linear layer, followed by a Relu Layer, a Dropout Layer, another Linear Layer and Relu Layer and a final Linear.
\end{enumerate}

We use Recall metric In order to assess the performance of regressor. We calculate 3 recall performances:
\begin{description}
\item [Cuboid Recall,] which is the recall performance for proposed cuboids. We interested in this metric because, we
  want to know how good are our proposals before modifying them into sequences of boxes.

\item [Single frame Recall,] which is the recall performance for the proposed ToI against the groundtruth tubes.
\item[Follow-up Single Frame Recall,] which is the recall performance for only the cuboids that were over the overlap threshold between
  proposed cuboids and groundtruth cuboids. We use this metric in order to know how many of our proposed cuboids end up in being good proposals.
\end{description}

\begin{table}[h]
  \centering
  \begin{tabular} {||c | c || c | c | c ||}
    \hline
    \textbf{Dataset} & \textbf{Pooling} & \textbf{Cuboid} & \textbf{Singl. Fr. } &  \textbf{Follow-up S.F.}\\
    \hline                
    \multirow{2}{*}{JHMDB} & avg & 0.8545 & 0.7649 & 0.7183 \\
    \cline{2-5}
    {} & max & 0.8396 & 0.7761 & 0.5783 \\
    \cline{1-5}
    \multirow{2}{*}{UCF} & avg & 0.5319 & 0.4694 & 0.5754 \\
    \cline{2-5}
    {} & max & 0.5190 & 0.5021 & 0.5972 \\
    \cline{1-5}
                                   
  \end{tabular}
  \caption{Recall results after converting cuboids into sequences of frames}
  \label{table:reg_1_1}
\end{table}

As the above results show, we get lower recall performance in frame-level. On top of that, when we translate a cuboid into
a sequence of boxes, we miss 20-40\% of our proposals. This means that we don't modify good enough our cuboids, although
we get only 10\% decrease. Probably, we get such score from cuboids, that even though, didn't overlap well (according to
overlap threshold), achieve to become a good proposal in frame-level and in temporal level. 


\subsection{Changing Regressor - from 3D to 2d}
After getting first recall results, we experiment using another architecture for the regressor network, in order to solve the modification
problem, introduced in the previous section. Instead of having a 3D Convolutional Layer, we will use a 2D Convolutional Layer
in order to treat the whole time dimension as one during convolution operation. So, as shown in Figure \ref{fig:reg_1_2},
the $2^{nd}$ Regression Network is about the same with first one, with 2 big differences:
\begin{enumerate}
\item We performing a pooling operation at the feature maps extracted by the 3D Roi Align operation, after we are normalized.
\item Instead of a 3D Convolutional Layer, we have a 2D Convolutional Layer with kernel size = 1, stride = 1 and no padding.
\end{enumerate}

\begin{figure}[h]

  \centering
  \includegraphics[scale=0.48]{regressor_1_2}
  \caption{Structure of Regressor}
  \label{fig:reg_1_2}
\end{figure}

On top of that, we tried to determine which feature map is the most suitable  for getting best-scoring recall performance. This feature map will be given as
input to the Roi Algin operation.  At Table \ref{table:reg_1_2}, we can see the recall performance for different feature maps and different pooling methods.

\begin{table}[h]
  \centering
  \begin{tabular}{||c | c | c || c  c  c ||}
    \hline
    \textbf{Dataset} & \textbf{Pooling} & \textbf{F. Map} & \textbf{Recall} &  \textbf{ Recall SR}  &  \textbf{Recall SRF} \\
    \hline
    \multirow{6}{*}{JHMDB} & \multirow{3}{*}{mean} & 64 &  0.6828  & 0.5112  & 0.7610 \\
    \cline{3-6}
    {} & {} & 128 & 0.8694 & 0.7799 & 0.6756 \\
    \cline{3-6}
    {} & {} & 256 & 0.8396 & 0.7687 & 0.7029 \\
    \cline{2-6}
    {} & \multirow{3}{*}{max} & 64 &  0.8582 & 0.7985 & 0.5914\\
    \cline{3-6}
    {} & {} & 128 & 0.8358 & 0.7724 & 0.8118 \\
    \cline{3-6}
    {} & {} & 256 & 0.8657 & 0.8022 & 0.7996 \\
    \hline
    \multirow{6}{*}{UCF} & \multirow{3}{*}{mean} & 64 & 0.5055 & 0.4286 & 0.5889 \\
    \cline{3-6}
    {} & {} & 128 & 0.5335 & 0.4894 & 0.5893 \\
    \cline{3-6}
    {} & {} & 256 & 0.5304 & 0.4990 & 0.6012 \\
    \cline{2-6}
    {} & \multirow{3}{*}{max} & 64 & 0.5186 & 0.4990 & 0.5708 \\
    \cline{3-6}
    {} & {} & 128 & 0.5260 & 0.4693 & 0.5513 \\
    \cline{3-6}
    {} & {} & 256 & 0.5176 & 0.4878 & 0.6399 \\
    \hline

  \end{tabular}
  \caption{Recall performance using 3 different feature maps as Regressor's input and 2 pooling methods}
  \label{table:reg_1_2}
\end{table}

As we noticed from the above results, again, our system has difficulty in translating cuboids into 2D sequence of ROIs.
So, that makes us rethink the way we designed our TPN.


\section{ 3D anchors as 4k-dim vector}
In this approach, we set 3D anchors as 4k coordinates (k = 16 frames = sample duration). So a typical anchor is written as ($x_1, y_1, x'_1, y'_1, x_2, y_2, ...$)
where $x_1, y_1, x'_1, y'_1 $ are the coordinates for the 1st frame, $x_2, y_2, x'_2, y'_2$ are the coordinates for the 2nd frame etc, as presented in \cite{DBLP:journals/corr/abs-1712-09184}.
In figure \ref{fig:anchor_4k} we can an example of this type of anchor.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{anchor_4k}
  \caption{An example of the anchor $(x_1,y_1,x'_1,y'_1,x_2,y_2, ...)$}
  \label{fig:anchor_4k}
\end{figure}

The main advantage of this approach is that we don't need to translate the 3D anchors into 2D boxes, which caused many problems at the previous approach.
However, it has a big drawback, which is the fact that this type of anchors has fixed time duration.
In order to deal with this problem, we set anchors with different time durations, which are 16, 12, 8 and 4.
Anchors with duration $ < $ sample duration (16 frames) can be written as 4k vector with zeroed coordinates in the frames bigger that the time duration. For example, an anchor with
2 frames duration, starting from the 2nd frame and ending at the 3rd can be written as (0, 0, 0, 0, $x_1, y_1, x'_1, y'_1, x_2, y_2, x'_2, y'_2$, 0, 0, 0, 0) if the sample
duration is 4 frames. 

\begin{figure}[h]
  \centering
  % \includegraphics[width=1.\textwidth]{tpn_2}
  \includegraphics[scale=0.4]{tpn_2}
  \caption{The structure of TPN according to new approach}
  \label{fig:New_structure}
\end{figure}

This new approach led us to change the structure of TPN. The new one  is presented in figure \ref{fig:New_structure}. As we can see, we \
added scoring and regression layers for each duration. So, TPN follows the upcoming steps in order to propose ToIs:
\begin{enumerate}
\item At first, we get the feature map, extracted by ResNet, as input to a 3D Convolutional Layer with kernel size = 1, stride = 1 and no padding.
\item From Convolutional Layer, we get as output an activation map with dimensions (256,16,7,7). For reducing time dimension, we use 4 pooling layer,
  one for each sample duration with kernel sizes \textit{(16,1,1), (12,1,1,), (8,1,1) and (4,1,1)} and stride = 1,  for sample durations 16, 12, 8 and 4 respectively.
  So, we get activation maps with dimensions \textit{(256,1,7,7), (256,5,7,7), (256,9,7,7) and (256,13,7,7)}, in which second dimension is the number of possible
  time variations. For example, in $(256,5,7,7)$ feature map, which is related with anchors with duration 12 frames, we can have 5 possible cases, from frame 0 to frame
  11, frame 1 to frame 12 etc.
  
\item Again, like in previous approach, for each pixel of the activate map we correspond \textbf{n = k = 15}
  anchors (5 scale of 1, 2, 4, 8, 16, 3 aspect rations of  1:1, 1:2, 2:1). Of course, we have 4 different activate maps, with 1, 5, 9 and 13
  different cases and a $7 \times 7$ shape in each filter. So, in total we have $28 \cdot 15 \cdot 49 = 20580$ different anchors.
  Respectively, we have 20580 different regression targets.

\end{enumerate}

\subsection{Training}
Training procedure stays almost the same like previous approach's. So, again, we randomly choose  a video segment and its corresponding groundtruth action tubes. But in this training procedure,
we consider anchors as foreground when they overlap more than 0.7 with any groundtruth tube, alongside with background anchors whose overlap is bigger that 0.1 and smaller than 0.3. We are not
concerned about the rest of the anchors.

\begin{table}[h]
  \centering
  \begin{tabular}{||c | c || c  c c||}
    \hline
    \textbf{Dataset} & \textbf{Pooling} &  \textbf{Recall(0.5)} & \textbf{Recall(0.4)} & \textbf{Recall(0.3)} \\
    \hline  \hline
    \multirow{2}{4em}{JHMDB} & mean & 0.6866 & 0.7687 & 0.8582 \\
    \cline{2-5}
    {} & max &  0.8134 & 0.8694 & 0.9216 \\
    \hline
    \multirow{2}{4em}{UCF} & avg &  0.5435 & 0.6326 & 0.7075 \\
    \cline{2-5}
    {} & max & 0.6418 & 0.7255 & 0.7898 \\
    \hline
  \end{tabular}
  \caption{Recall results using 2nd approach for anchors}
  \label{table:tpn_2_1}
\end{table}

As Table \ref{table:tpn_2_1} shows, it is obvious that we get better recall performances compared to previous' approach.
Additionally, we can see that 3D max pooling performs better than 3D avg pooling. The difference
between max pooling and avg pooling is about 10\%, which is big enough to make us choose max pooling operation as pooling method before getting anchors' scores
and regression targets.

\subsection{Adding regressor}

Even though, our TPN outputs frame-level boxes, we need to improve these predictions in order to overlap
with the groundtruth boxes as well as possible.
So, in full correspondence with the previous approach, we added an regressor for trying to get better recall results.

\paragraph{3D Roi align}
In this approach, we know already the 2D coordinates. So, we can use the method proposed from \cite{DBLP:journals/corr/abs-1712-09184}. They
extend RoiAlign operator by splitting the tube into $T$ 2D boxes. Then, they use classic RoiAlign to extract a region from each one 
of the temporal slices in the feature map. After that, they concatenate the region in time domain so they get a $T \times R \times R$
feature map, where $R$ is the output resolution of RoiAlign, which is 7 in our situation. \par

As a first approach, we use a 3D convolutional layer, followed by 2 linear layers. Our regressors follows the following steps:
\begin{enumerate}
\item At first, use 3D RoiAlign in order to extract the feature maps of the proposed action tubes. We normalize them, and give them as input to the 3D
  convolutional layer.
\item The output of the 3D Convolutional Layer is fed into 2 Linear layers with ReLu faction between them and finally we get $sample duration \times 4$
  regression targets. We keep only the proposed targets, that there is a corresponding 2D box.
\end{enumerate}


We train our regressor using the same loss function as previous approach's formula which is:
\begin{equation*} 
\begin{split}
 L  =  \sum_iL_{cls}(p_i, p_i^*) + \sum_iL_{cls}(p_{fixed,i}, p_{fixed,i}^*) + \\
 \sum_ip_i^*L_{reg}(t_i,t_i^*) + \sum_ip_{fixed,i}^*L_{reg}(t_{fixed,i},t_{fixed,i}^*) + \\
  \sum_iq_i^*L_{reg}(c_{i}, c_{i}^*) + \\
  % \sum_iL_{cls}(p_i, p_i^*) + \sum_iL_{cls}(p_{fixed,i}, p_{fixed,i}^*) 
\end{split}
\end{equation*}

We want again to find the best matching feature maps, so we train our regressor for feature maps
$(64,8,7,7)$ and $(128,8,7,7)$. We didn't experiment using $(256,8,7,7)$ feature map because
we got OutOfMemory error during training, despite several modifications we did in the
implementation code.

\begin{table}[h]
  \centering
  \begin{tabular}{||c | c || c  c  c||}
    \hline
    \textbf{Dataset} & \textbf{Feat. Map} & \textbf{Recall(0.5)} & \textbf{Recall(0.4)} & \textbf{Recall(0.3)}\\
    \hline
    \multirow{2}{*}{JHMDB} &  64 & 0.7985 & 0.903 & 0.9552 \\
    \cline{2-5}
    {} & 128 & 0.7836 & 0.8881 & 0.944\\
    \hline
    \multirow{2}{*}{UCF}  & 64 & 0.5794 & 0.7206 & 0.8134 \\
    \cline{2-5}
    {} & 128 & 0.5622 & 0.7204 & 0.799 \\
    \hline

  \end{tabular}
  \caption{Recall performance when using a 3D Convolutional Layer in Regressor's architecture}
  \label{table:reg_2_1}
\end{table}

According to Table \ref{table:reg_2_1}, we got the best results when we use $(64,16,7,7)$ feature map. This is the expected result, because these feature maps
are closer to the actual pixels of the actor, than $(128,16,7,7)$ feature maps, in which because of $3 \times 3 \times 3$ kernels, which combine spatiotemporal
information from neighbour pixels. However, as we can see, we got worst recall performance than when we didn't use any regressor if we compare the results from Tables
\ref{table:tpn_2_1} and \ref{table:reg_2_1}.

\subsection{From 3D to 2D}

Following the steps we used before, we design an architecture that uses instead of a 3D Convolutional Layer, a 2D. Unlike we did before, in this case, we haven't 
used any pooling operation before feeding the first 2D Convolutional Layer. On the contrary, we manipulate our feature maps like not being spatiotemporal but,
only spatial. So, our steps are:
\begin{enumerate}
\item At first, we use, again ,3D RoiAlign in order to extract the feature maps of the proposed action tubes and normalize them. Let us consider a feature map
  extracted from ResNet, which has dimensions $(64,sample duration,7,7)$ and after applying RoiAlign and normalization, we get a $(k,64,sample duration,7,7)$ feature map,
  where \textit{k} is the number of proposed  action tubes for this video segment.
\item We slice the proposed action tubes into T 2D boxes, so the dimensions of the Tensor, which contains the coordinates of action tubes, from $(k,4\cdot sample duration)$
  become $(k,sample duration, 4)$. We reshape the Tensor into $(k\cdot sample duration, 4)$, in which, first k coordinates refer to the first frame, the
  second k coordinates refer to the second frame and so on.
\item Respectively, we reshape extracted feature maps from $(k, 64, sample duration, 7, 7)$ to $(k\cdot sample duration, 64, 7, 7)$. So, now we deal with 2D feature maps, for which as we said before,
  we consider that contain only spatial information. So, we use 3 Linear Layers in order to get 4 regression targets. We keep only those we have a corresponding bounding
  box.
\end{enumerate}

Again, we experiment using 64, 128 and 256 feature maps (in this case, there is no memory problem). The results of our experiments are shown in Table \ref{table:reg_2_2}.
% After getting first recall results, we experiment using another architecture for the regressor network. Our idea was that we can treat feature maps like not having
% temporal dependencies between their frames. So, at first, we get from Roi Align operation activation maps with dimensions \textit{(k,64,16,7,7)} where k is the number
% of ToIs proposed for this video segment. We reshape this feature map to \textit{ (k*16,64,7,7) }, and in same time, we reshape their proposed action tubes from \textit{(k,16,4)}
% to \textit{(k*16,4)}. So, the new regression Network is consisted with:
% % \begin{enumerate}
% % \item a 2D Convolutional Network
% \paragraph{From 3D to 2d}
% The first idea we thought, was to change the first Convolutional layer from 3D to 2D. This means that we consider  features  not to have temporal dependencies for
% each frame. As we can see in the figure \ref{}, we got worse results, so, we rejected this idea.

% \subsection{Trying to  improve performance}
% TODO
% \subsection{Changing training procedure}
% TODO

\begin{table}[h]
  \centering
  \begin{tabular}{||c | c || c  c  c||}
    \hline
    \textbf{Dataset}  & \textbf{Feat. Map} & \textbf{Recall(0.5)} & \textbf{Recall(0.4)} & \textbf{Recall(0.3)}\\
    \hline
    \multirow{3}{*}{JHMDB} & 64 & 0.8358 & 0.9216 & 0.9739\\
    \cline{2-5}
    {} & 128 & 0.8172 & 0.9142 & 0.9627 \\
    \cline{2-5}
    {} & 256 & 0.7724 & 0.8731 & 0.9328 \\
    \hline
    \multirow{3}{*}{UCF} & 64 & 0.6368 & 0.7346 & 0.7737 \\ 
    \cline{2-5}
    {} & 128 & 0.6363 & 0.7133 & 0.7822 \\
    \cline{2-5}
    {} & 256 &  0.6363 & 0.7295 & 0.7822 \\
    \hline

  \end{tabular}
  \caption{Recall performance when using a 2D Convolutional Layer instead of 3D in Regressor's model}
  \label{table:reg_2_2}
\end{table}

As we can see, we get improved recall performance up 3\% for JHMDB dataset and about the same performance for UCF dataset. Again, we get best performance
if we choose $(64, 16, 7, 7)$ feature maps.

\subsection{Changing sample duration}
After trying all the previous versions, we noticed that we get about the same recall performances with some small improvements. So, we thought that we could try
to reduce the sample duration. This idea is based on the fact that reducing sample duration, means that anchor dimensions will reduce, so the number of
candidate anchors. That's because, now we have a smaller number of cases, so smaller number of parameters alongside with a small number of dimensions for regression targets.
We train our TPN for sample duration = 8 frames 4 frames. We use, of course, TPN's second  architecture, because as shown before, we get better recall performance.

\subsubsection{Without Regressor}

At first, we train TPN, again without regressor. We do so, in order to compare recall performance for all sample durations, without using any regressor. The results
are shown in Table \ref{table:new_sample}. For all cases, we use max pooling before scoring and regression layers, and we didn't experiment at all with
avg pooling. Of course, for sample duration = 16, we used the calculated one in  Table \ref{table:tpn_2_1}.

\begin{table}[h]
  \centering
  \begin{tabular}{|c | c || c c c|}
    \hline
    \textbf{Dataset} & \textbf{Sample dur} & \textbf{Recall(0.5)} &  \textbf{Recall(0.4)} &  \textbf{Recall(0.3)} \\
    \hline
    \multirow{3}{*}{JHMDB} & 16 & 0.8134 & 0.8694 & 0.9216 \\
    \cline{2-5}
    {} & 8 & 0.9515 & 0.9888 & 1.0000 \\
    \cline{2-5}
    {} & 4 & 0.8843 & 0.9627 & 0.9888 \\
    \hline
    \multirow{3}{*}{UCF} & 16 & 0.6418 & 0.7255 & 0.7898 \\
    \cline{2-5}
    {} & 8 & 0.7942 & 0.8877 & 0.9324\\
    \cline{2-5}
    {} & 4 & 0.7879 & 0.8924 & 0.9462 \\
    \hline
    
  \end{tabular}
  \caption{Recall results when reducing sample duration to 4 and 8 frames per video segment}
  \label{table:new_sample}
\end{table}

According to Table \ref{table:new_sample}, we notice that we get best performance for sample duration = 8 for both datasets. For dataset JHMDB sample duration equal to 8
gets far better results from the other approaches, followed by approach with sample duration = 4. For UCF dataset, although sample duration equal to 8 gives us best performances
sample duration equal to 4 gives us about the same. The difference between those 2 duration is less that 1\%. 

\subsubsection{With Regressor}                                             

Following the idea of reducing the sample duration for getting better recall performance, we trained TPN with a regressor. We trained for both approaches, which means both 3D and 2D Convolutional
Layer approaches were trained. Recall performances are presented in Table \ref{table:new_sample_reg}.

\begin{table}[h]
  \centering
  \begin{tabular}{|c | c | c || c c c|}
    \hline
    \textbf{Dataset} & \textbf{Sample dur} & \textbf{Type} & \textbf{Recall(0.5)} &  \textbf{Recall(0.4)} &  \textbf{Recall(0.3)} \\
    \hline
    \multirow{4}{*}{UCF} & \multirow{2}{*}{8} & 2D & 0.8078 & 0.8870 & 0.9419 \\
    \cline{3-6}
    {} & {} & 3D & 0.8193 & 0.8930 & 0.9487 \\
    \cline{2-6}
    {} & \multirow{2}{*}{4}& 2D & 0.7785 & 0.8914 & 0.9457 \\
    \cline{3-6}
    {} & {} & 3D & 0.7449 & 0.8605 & 0.9362 \\
    \hline
    \multirow{4}{*}{JHDMBD} & \multirow{2}{*}{8} & 2D &  0.9366 & 0.9851 & 0.9925  \\
    \cline{3-6}
    {} & {} & 3D & 0.8918 & 0.9776 & 0.9963  \\ 
    \cline{2-6}
    {} & \multirow{2}{*}{4}& 2D & 0.9552 & 0.9963 & 1.0000 \\
    \cline{3-6}
    {} & {} & 3D & 0.9142 & 0.9701 & 0.9888  \\
    \hline
    
  \end{tabular}
  \caption{Recall results when a regressor and sample duration equal to 4 or 8 frames per video segment}
  \label{table:new_sample_reg}
\end{table}

According to \ref{table:new_sample_reg}, it is clear that using a 2D Convolutional Layer as presented above results in better recall performance that using a 3D. Furthermore, we notice that
the addition of a regressor causes both improvements and deteriorations in recall performances. For dataset UCF, approach with sample duration = 8 improves by about 1-2\% recall performance,
but for sample duration = 4 it reduce it by 1-3\%. On the other hand, for dataset JHMDB, now, sample duration = 4 gets better results by adding a regressor and sample duration = 8 gets
worse. So, after considering both results from Tables \ref{table:new_sample} and \ref{table:new_sample_reg}, we think that the best approach is using sample duration equal to 8, with the
addition of a regressor, which uses a 2D Convolutional Layer. We know that this approach gets worse performance at JHMDB but it gives us the best results in UCF. But, since JHMDB's results are
high enough, we are most interested in improving UCF's results. That's the reason, we will use the aforementioned approach in the rest chapters.

\section{General comments}

In the previous section, we presented 2 different approaches for proposing sequences of bounding boxes which are likely to include an
actor performing an action. After considering both approaches, we deduce that the second approach results in better proposals
according to their recall performance. In Figure \ref{fig:proposals}, 3 different generated ToIs are presented.

\begin{figure}[h]
  \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[width= 0.8\textwidth, height=0.1\textheight]{proposals0_half}
    \caption{}
    \label{fig:proposals0}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[width= 0.8\textwidth, height=0.1\textheight]{proposals1_half}
    \caption{}
    \label{fig:proposals1}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[width= 0.8\textwidth, height=0.1\textheight]{proposals2_half}
    \caption{}
    \label{fig:proposals2}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[width= 0.8\textwidth, height=0.1\textheight]{proposal_gt}
    \caption{}
    \label{fig:proposals_gt0}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[width= 0.8\textwidth, height=0.1\textheight]{proposal_gt1}
    \caption{}
    \label{fig:proposals_gt1}
  \end{subfigure}
  \centering
  % \caption{(\protect\subref{fig:proposals0}),(\protect\subref{fig:proposals1}),(\protect\subref{fig:proposals2})
  %   present 3 ToIs while (\protect\subref{fig:proposals_gt0}),(\protect\subref{fig:proposals_gt1})
  %   present first 2 proposals with their corresponding grouthtruth ToI}
  \caption{ Visualization of Tois including grouthtruth action tubes, too }


  \label{fig:proposals}
\end{figure}

Our goal is  the bounding boxes to overlap with the actor precisely. According to Figure \ref{fig:proposals}, all of three proposed actions tubes
presented in Figures \ref{fig:proposals0}, \ref{fig:proposals1} and \ref{fig:proposals2} overlap with the actor to some degree.
However, by looking, it is clear that ToI, presented in Figure \ref{fig:proposals1}, doesn't overlap very well like those appearing at Figures \ref{fig:proposals0} and \ref{fig:proposals2}
Intuitively, comparing proposals from \ref{fig:proposals0} and \ref{fig:proposals2}, we think that \ref{fig:proposals0} overlaps better than \ref{fig:proposals2}.

However, as shown in Figures \ref{fig:proposals_gt0} and \ref{fig:proposals_gt1}, the second ToI better overlaps with the groundtruth. It is clear that even though
the first proposal overlaps very well with the upper body of the actor, it fails to capture the left leg, which may be a crucial factor for determining the class of the
action. The ToI shown in Figure \ref{fig:proposals_gt1} manages to capture most of the actor's body, excluding only the head, which usually doesn't move a lot
during an action. Although, judging intuitively, we would choose the first ToI, in reality, the second one is more preferable.

% \end{document}

