\documentclass{article}

\usepackage{mathtools}
% \usepackage{amsmath}

\begin{document}

Smooth L1-loss can be interpreted as a combination of L1-loss and L2-loss. It behaves as L1-loss when the absolute
value of the argument is high, and it behaves like L2-loss when the absolute value of the argument is close to zero.
The equation is : \\

% \[ L\textsubscript{1 : smooth}  =
% \left \{ 
% \begin{tabular}{ccc}
%   \(\lvert x\rvert\) && if  \(\lvert x\rvert\) $>$ $\alpha$  \\
%   $\frac{1}{   \lvert { \alpha } \rvert   } $ $x^2$ &&   if \(\lvert x\rvert\) $\leq$  $\alpha$
%   \end{tabular}
% \right .
% \]
L\textsubscript{1 : smooth}  = $\begin{cases}
  \lvert {x}\rvert & \mbox{ if } \lvert { x }\rvert \mbox{ $>$ $\alpha$ } \\ 
  \frac{1}{   \lvert { \alpha } \rvert   }  x^2  &   \mbox{ if }  \lvert {x}\rvert \mbox { $\leq$  $\alpha$ }
\end{cases}$
\\ \\
$\alpha$ is a hyper-parameter here and is usually taken as 1. $\frac{1}{ \lvert { \alpha } \rvert } $ appears near to $x^2$ term
to make it continuous.
\\ \\
Smooth L1-loss combines the advantages of L1-loss (steady gradients for large values of \textit{x}) and L2-loss (less oscillations during
updates when \textit{x} is small).The smooth L1-loss is used for doing box regression on some object detection systems, (SSD, Fast/Faster RCNN). According to those papers this loss is less sensitive to  outliers, than other regression loss, like L2 which is used on R-CNN. \\


\textbf{Why we set sigma equal to  3?} \\

As sigma $\, \to \,$  inf the loss approaches L1 (abs) loss. Setting sigma = 3, makes the transition point from quadratic to linear happen
at $\lvert{x} \rvert$ $\leq$ 1 / 3**2 (closer to the origin). The reason for doing this is because the RPN bbox regression targets are not normalized
by their stdev (unlike in Fast R-CNN), because the statistics of the targets are changing constantly throughout learning.
% In a future update I may simply replace smooth L1 with (hard) L1 which I believe will likely work as well and be simpler (no sigma, etc.).

\end{document}