% \documentclass{report}

% \usepackage{hyperref}  % package for linking figures etc
% \usepackage{enumitem}  % package for description with bullets
% \usepackage{graphicx}  % package for importing images
% \usepackage{mathtools} % package for math equation
% \usepackage{mathrsfs}  % package for math font
% \usepackage{indentfirst} % package for getting ident after section or paragraph
% % \usepackage{amsmath}
% \usepackage[
%     backend=bibtex,
%     style=authoryear,
%     sorting=ynt,
%     % style=numeric,
%     % style=alphabetic ,
%   ]{biblatex}
 
%  \addbibresource{References}
% \setlength{\parindent}{2em} % how much indent to use when we start a paragraph

% \graphicspath{ {./theory/figures/} }       % path for images

% \begin{document}

\chapter{Related work}


% The combination of CNN and Recurrent Neural Network (RNN) is a method widely been studied and achieved excellent results.
% The recently proposed methods can be seperated into 2 subcategories: (1) use a CNN detector in order to localize directly
% the action instance for each single frame and (2) modify the detector in order to get as input volute multiple frames and
% use 3D convolution.%  Both approaches produce the same type of output : i) an actioness score of a bounding box or a proposed
% tube ii) coordinate offsets for bounding box refinement and iii) clasification probabilities for all action classes.

% The task of action classification in approaches used features based on shape or motion sucha as HOG \cite{dalal2005histogramcvpr},
% SIFT \cite{Lowe2004}, MBH \cite{dalal2006human} and train a classifier as SVM.

% \paragraph{Frame based methods}
\section{Action Recognition}
First approaches for action classification consisted of 2 steps a) compute complex handcrafted features from raw video frames
such as SIFT, HOG, optical flow and b) train a classifier based on those features. These approaches made the choise of
features a signifact factor for network's performance. That's because different action classes may appear dramatically
different in terms of their appearences and motion patterns. Another problem was that most of those approaches take
assumptions about the circumstances under which the video was taken because there was problems such as cluttered
background, camera viewpoit variations etc. A review techniques used until 2011 made by \cite{Aggarwal:2011:HAA:1922649.1922653}.

Recent results in deep architectures and especially in image classification made us attempt to train CNN networks for
the task of action classification. First significant attempt made by \cite{6909619}.  % \cite{DBLP:journals/corr/WangXW0LTG16}.
 \cite{simonyan2014two} and \cite{DBLP:journals/corr/FeichtenhoferPZ16} both added optical flow in order to achieve better results.
On top of that, the increase  in computing performance contributed to the design more complicated architectures including
3D Convolutions as presented in \cite{6165309} as done by \cite{DBLP:journals/corr/TranBFTP14}.

R(2+1) \cite{DBLP:journals/corr/abs-1711-11248}
\textbf(Pending...More before 3D ResNet)
Recent day 3D ResNet has been introduced by \cite{Hara_2018_CVPR} 
\section{Action Localization}
As mentioned before, Action Localization can be seen as an extention of object detection problem, where the outputs are action tubes
that consist of a sequence of bounding boxes. So, there are several approaches including an object-detector network for single frame
action proposal and a classifier. The introduction of R-CNN (\cite{DBLP:journals/corr/GirshickDDM13}) achieve significant improvemet
in the performance of Object Detection Networks. This architectures, firstly, proposes regions in the image which are likely to
contain an object and then it classify it using a SVM classifier. Inspired by this architecture, \cite{DBLP:journals/corr/GkioxariM14}
desing a 2-stream RCNN network in order to generate action proposals for each frame, one stream for frame level and one for optical flow.
Then they  connect them using viterbi connection algorithm. \cite{DBLP:journals/corr/WeinzaepfelHS15} extend this approach, performing
frame-level proposals and using a tracker for connecting those proposals using both spatial and optical flow features. Also it performs
temporal localization using a sliding window over the tracked tubes. \cite{peng:hal-01349107} used Faster R-CNN (\cite{Ren:2015:FRT:2969239.2969250})
instead of RCNN for frame-level proposals, and they use Viterbi algorithm for linking proposals, too. For temporal localization, they
use a maximum subarray method.







\cite{6909495} introduces the tubelets.

% \\
% \cite{peng:hal-01349107} NA DW TI KANEI.

\cite{singh2016online} uses SSD 

Some approaches include tracking \cite{DBLP:journals/corr/WeinzaepfelHS15}.
Other approaches treat a video as a sequence of frames such as in \cite{DBLP:journals/corr/KalogeitonWFS17} and in \cite{DBLP:journals/corr/HouCS17}.

\paragraph{3d-2d pose}



% % \paragraph{Tube-level proposals}
% The previous approaches do not consider the motion information enconded in multiple contiguous frames because they treat video
% frames as still images. In \cite{pmid:22392705} was introuduce 3D Convolution which captures spatial and temporal information.

% On top of that, and based also in \cite{simonyan2014two},  \cite{DBLP:journals/corr/KalogeitonWFS17} presented action tubes

% % We can devide
% % the previous methods in 3 subcategories (1) temporal modeling for action representation (2) object detection and (3) spatial-temporal
% % localization.


\section{Our implementation}
We propose a network similar to \cite{DBLP:journals/corr/HouCS17}. Our architecture is consisted by the following basic elements:
\begin{itemize}
\item One 3D Convolutional Network, which is used for feature extraction. In our implementaion we use a 3D Resnet network which is taken from
  \cite{hara3dcnns} and it is based on ResNet CNNs for Image Classification \cite{DBLP:journals/corr/HeZRS15}.
\item Tube Proposal Network for proposing action tubes (based on the idea presented in \cite{DBLP:journals/corr/HouCS17}).
\item A classifier for classifying video tubes.
\end{itemize}
  


% \printbibliography

% \end{document}